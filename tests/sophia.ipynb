{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from src.methods import SophiaG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import os\n",
    "\n",
    "print('Downloading dataset')\n",
    "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
    "\n",
    "if not os.path.exists('./cola_public_1.1.zip'):\n",
    "  wget.download(url, './cola_public_1.1.zip')\n",
    "  if not os.path.exists('./cola_public/'):\n",
    "    !unzip cola_public_1.1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t',\n",
    "                 header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "for sent in df.sentence.values:\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent, \n",
    "                        add_special_tokens = True, \n",
    "                   )\n",
    "    input_ids.append(encoded_sent)\n",
    "print('Original: ', df.sentence.values[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(seqs, maxlen=None, value=0, padding=\"post\"):\n",
    "    if maxlen is None:\n",
    "        raise ValueError(\"Invalid maxlen: {}\".format(maxlen))\n",
    "    for i in range(len(seqs)):\n",
    "        add = [value] * max(0, maxlen - len(seqs[i]))\n",
    "        if padding == \"post\":\n",
    "            seqs[i] = seqs[i] + add\n",
    "        elif padding == \"pre\":\n",
    "            seqs[i] = add + seqs[i]\n",
    "    return seqs\n",
    "\n",
    "MAX_LEN =  max([len(sen) for sen in input_ids])+1\n",
    "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
    "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
    "\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, value=0, padding=\"post\")\n",
    "print('\\Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks = []\n",
    "for sent in input_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    attention_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.pcolor(attention_masks[1000:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "data_split_rs = 49\n",
    "\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids,df.label.values,\n",
    "                                                            random_state=data_split_rs, test_size=0.1)\n",
    "\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, df.label.values,\n",
    "                                             random_state=data_split_rs, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.configs import set_one_device\n",
    "\n",
    "device = set_one_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "random_seed = 4\n",
    "num_workers = 1\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "# ###full_grad_dataloader\n",
    "# batch_size = 32 # train_inputs.shape[0]\n",
    "# # Create the DataLoader for our training set.\n",
    "# train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "# train_sampler = RandomSampler(train_data)\n",
    "# train_loader_for_full_grad = DataLoader(\n",
    "#     train_data, batch_size=batch_size, num_workers=num_workers,\n",
    "#     worker_init_fn = lambda id: np.random.seed(id + num_workers * random_seed)\n",
    "# )\n",
    "\n",
    "\n",
    "### for training\n",
    "batch_size = 32\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_loader = DataLoader(\n",
    "    train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers,\n",
    "    worker_init_fn = lambda id: np.random.seed(id + num_workers * random_seed)\n",
    ")\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "val_loader = DataLoader(\n",
    "    validation_data, batch_size=batch_size, shuffle=True, num_workers=num_workers,\n",
    "    worker_init_fn = lambda id: np.random.seed(id + num_workers * random_seed)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = []\n",
    "\n",
    "history_random_seed = 74\n",
    "\n",
    "for starting_point_random_seed in [73, 74]:\n",
    "    for _ in range(4):\n",
    "        torch.manual_seed(starting_point_random_seed)\n",
    "        nets.append(\n",
    "            BertForSequenceClassification.from_pretrained(\n",
    "                \"bert-base-uncased\",\n",
    "                num_labels = 2,\n",
    "                output_attentions = False,\n",
    "                output_hidden_states = False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        nets[-1].zero_grad()\n",
    "        for i, name_param in enumerate(nets[-1].named_parameters()):\n",
    "            if i < 197: # 197 -- 2 linears, 199 - only last linear\n",
    "                name_param[1].requires_grad = False\n",
    "\n",
    "        nets[-1].train()\n",
    "\n",
    "torch.manual_seed(history_random_seed)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
