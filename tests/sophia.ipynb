{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lion_pytorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#from src.methods import SophiaG\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlion_pytorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Lion\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lion_pytorch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from src.methods import SophiaG\n",
    "from lion_pytorch import Lion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import os\n",
    "\n",
    "print('Downloading dataset')\n",
    "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
    "\n",
    "if not os.path.exists('./cola_public_1.1.zip'):\n",
    "  wget.download(url, './cola_public_1.1.zip')\n",
    "  if not os.path.exists('./cola_public/'):\n",
    "    !unzip cola_public_1.1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t',\n",
    "                 header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "for sent in df.sentence.values:\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent, \n",
    "                        add_special_tokens = True, \n",
    "                   )\n",
    "    input_ids.append(encoded_sent)\n",
    "print('Original: ', df.sentence.values[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(seqs, maxlen=None, value=0, padding=\"post\"):\n",
    "    if maxlen is None:\n",
    "        raise ValueError(\"Invalid maxlen: {}\".format(maxlen))\n",
    "    for i in range(len(seqs)):\n",
    "        add = [value] * max(0, maxlen - len(seqs[i]))\n",
    "        if padding == \"post\":\n",
    "            seqs[i] = seqs[i] + add\n",
    "        elif padding == \"pre\":\n",
    "            seqs[i] = add + seqs[i]\n",
    "    return seqs\n",
    "\n",
    "MAX_LEN =  max([len(sen) for sen in input_ids])+1\n",
    "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
    "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
    "\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, value=0, padding=\"post\")\n",
    "print('\\Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks = []\n",
    "for sent in input_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    attention_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.pcolor(attention_masks[1000:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "data_split_rs = 49\n",
    "\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids,df.label.values,\n",
    "                                                            random_state=data_split_rs, test_size=0.1)\n",
    "\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, df.label.values,\n",
    "                                             random_state=data_split_rs, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.configs import set_one_device\n",
    "\n",
    "device = set_one_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "random_seed = 4\n",
    "num_workers = 1\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "# ###full_grad_dataloader\n",
    "# batch_size = 32 # train_inputs.shape[0]\n",
    "# # Create the DataLoader for our training set.\n",
    "# train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "# train_sampler = RandomSampler(train_data)\n",
    "# train_loader_for_full_grad = DataLoader(\n",
    "#     train_data, batch_size=batch_size, num_workers=num_workers,\n",
    "#     worker_init_fn = lambda id: np.random.seed(id + num_workers * random_seed)\n",
    "# )\n",
    "\n",
    "\n",
    "### for training\n",
    "batch_size = 32\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_loader = DataLoader(\n",
    "    train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers,\n",
    "    worker_init_fn = lambda id: np.random.seed(id + num_workers * random_seed)\n",
    ")\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "val_loader = DataLoader(\n",
    "    validation_data, batch_size=batch_size, shuffle=True, num_workers=num_workers,\n",
    "    worker_init_fn = lambda id: np.random.seed(id + num_workers * random_seed)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = []\n",
    "\n",
    "history_random_seed = 74\n",
    "\n",
    "for starting_point_random_seed in [73, 74]:\n",
    "    for _ in range(4):\n",
    "        torch.manual_seed(starting_point_random_seed)\n",
    "        nets.append(\n",
    "            BertForSequenceClassification.from_pretrained(\n",
    "                \"bert-base-uncased\",\n",
    "                num_labels = 2,\n",
    "                output_attentions = False,\n",
    "                output_hidden_states = False,\n",
    "            )\n",
    "        )\n",
    "        nets[-1].zero_grad()\n",
    "        for i, name_param in enumerate(nets[-1].named_parameters()):\n",
    "            if i < 197: # 197 -- 2 linears, 199 - only last linear\n",
    "                name_param[1].requires_grad = False\n",
    "        nets[-1].train()\n",
    "\n",
    "torch.manual_seed(history_random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = []\n",
    "\n",
    "for i in range(2):\n",
    "    opts += [\n",
    "        torch.optim.Adam(nets[i * 4 + 0].parameters(), lr = 6e-4, eps = 1e-8, weight_decay=0.0005),\n",
    "        torch.optim.SGD(nets[i * 4 + 1].parameters(), lr=6e-4, momentum=0.9),\n",
    "        SophiaG(nets[i * 4 + 2].parameters(),  lr=6e-4, betas=(0.965, 0.99), rho=0.01, weight_decay=1e-1),\n",
    "        Lion(nets[i * 4 + 3].parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "    ]\n",
    "\n",
    "opt_names = [ \n",
    "    \"Adam, 6e-4\",\n",
    "    \"SGD, 6e-4\",\n",
    "    \"Sophia-G, 6e-4\",\n",
    "    \"Lion, 1e-4\",\n",
    "] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_muls = [\n",
    "    1, 1,\n",
    "    1, 1\n",
    "] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = []\n",
    "\n",
    "for (net, optimizer, opt_name, bs_mul) in zip(nets, opts, opt_names, bs_muls):\n",
    "    hist.append({\n",
    "        \"task_name\": \"BERT on CoLA\",\n",
    "        \"name\": opt_name,\n",
    "        \"bs_mul\": bs_mul,\n",
    "        \"train_loss\": [], \"train_x\": [],\n",
    "        \"val_loss\": [], \"val_x\": [],\n",
    "        \"train_acc_top_1\": [], \"train_acc_top_5\": [],\n",
    "        \"val_acc_top_1\": [], \"val_acc_top_5\": [],\n",
    "        \"epochs_x\": [],\n",
    "        \"total_steps\": 0,\n",
    "        \"prev_val_eval_step\": 0,\n",
    "        \"batch_end\": True\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for optimizer in opts:\n",
    "    for state in optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred)).contiguous()\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_to(param, device):\n",
    "    if isinstance(param, torch.Tensor):\n",
    "        param.data = param.data.to(device)\n",
    "        if param._grad is not None:\n",
    "            param._grad.data = param._grad.data.to(device)\n",
    "    elif isinstance(param, dict):\n",
    "        for subparam in param.values():\n",
    "            recursive_to(subparam, device)\n",
    "    elif isinstance(param, list):\n",
    "        for subparam in param:\n",
    "            recursive_to(subparam, device)\n",
    "\n",
    "def optimizer_to(optim, device):\n",
    "    for param_group in optim.param_groups:\n",
    "        for param in param_group.values():\n",
    "            recursive_to(param, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sophia loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_bs = len(train_loader)\n",
    "block_size = 1024\n",
    "bs = total_bs * block_size # 5120 iby default in original implementation\n",
    "# iter_num = -1\n",
    "default_loss_arr, sampled_loss_arr = [], []\n",
    "\n",
    "nets[0].to(device)\n",
    "for epoch in range(10):\n",
    "    for iter_num, data in enumerate(train_loader, 0):\n",
    "        inputs, masks, labels = data[0].to(device), data[1].to(device), data[2].to(device)\n",
    "        outputs = net(\n",
    "            inputs,\n",
    "            token_type_ids=None,\n",
    "            attention_mask=masks,\n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step(bs=bs)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        default_loss_arr.append(loss.item())\n",
    "\n",
    "        if iter_num % 10 != 9:\n",
    "            continue\n",
    "        else:\n",
    "            logits = outputs.logits\n",
    "            samp_dist = torch.distributions.Categorical(logits=logits)\n",
    "            y_sample = samp_dist.sample()\n",
    "            loss_sampled = F.cross_entropy(logits.view(-1, logits.size(-1)), y_sample.view(-1), ignore_index=-1)\n",
    "            loss_sampled.backward()\n",
    "            optimizer.update_hessian()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            sampled_loss_arr.append(loss_sampled.item())\n",
    "            nets[0].zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**general loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "batch_mul_step_count = 400\n",
    "calc_norm_diffs = True\n",
    "\n",
    "for epoch in range(10):\n",
    "    for (net, optimizer, net_hist) in zip(nets, opts, hist):\n",
    "        net.to(device)\n",
    "        optimizer_to(optimizer, device)\n",
    "\n",
    "        total_steps = net_hist[\"total_steps\"]\n",
    "        bs_mul = net_hist[\"bs_mul\"]\n",
    "\n",
    "        if net_hist[\"bs_mul\"] == \"linear\":\n",
    "            if not (\"bs_mul_value\" in net_hist):\n",
    "                net_hist[\"bs_mul_value\"] = 1\n",
    "\n",
    "            bs_mul = net_hist[\"bs_mul_value\"]\n",
    "\n",
    "        net_hist[\"epochs_x\"].append(total_steps)\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "                net.train()\n",
    "\n",
    "                net_hist[\"batch_end\"] = False\n",
    "                inputs, masks, labels = data[0].to(device), data[1].to(device), data[2].to(device)\n",
    "\n",
    "                outputs = net(\n",
    "                    inputs,\n",
    "                    token_type_ids=None,\n",
    "                    attention_mask=masks,\n",
    "                    labels=labels\n",
    "                )\n",
    "                loss = outputs[0] / bs_mul\n",
    "                loss.backward()\n",
    "\n",
    "                if total_steps % bs_mul == bs_mul - 1:\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    net_hist[\"batch_end\"] = True\n",
    "\n",
    "                net_hist[\"train_loss\"].append(loss.detach().cpu().item() * bs_mul)\n",
    "                net_hist[\"train_x\"].append(total_steps)\n",
    "\n",
    "                if total_steps % bs_mul == bs_mul - 1:\n",
    "                    if net_hist[\"bs_mul\"] == \"linear\":\n",
    "                        net_hist[\"bs_mul_value\"] = int(int(total_steps) / batch_mul_step_count) + 1\n",
    "                        bs_mul = net_hist[\"bs_mul_value\"]\n",
    "\n",
    "                top_1 = accuracy(outputs.logits, labels.data, topk=(1,))\n",
    "                net_hist[\"train_acc_top_1\"].append(top_1[0].detach().cpu().item())\n",
    "\n",
    "                # evaluate on validation dataset\n",
    "                prev_val_eval_step = net_hist[\"prev_val_eval_step\"]\n",
    "                if (total_steps - prev_val_eval_step) > 20 and net_hist[\"batch_end\"]:\n",
    "                    net_hist[\"prev_val_eval_step\"] = total_steps\n",
    "\n",
    "                    net.eval()\n",
    "\n",
    "                    val_losses = []\n",
    "                    val_accs = []\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        for step, val_data in enumerate(val_loader):\n",
    "\n",
    "                            inputs, masks, labels = val_data[0].to(device), val_data[1].to(device), val_data[2].to(device)\n",
    "\n",
    "                            outputs = net(\n",
    "                                inputs,\n",
    "                                token_type_ids=None,\n",
    "                                attention_mask=masks,\n",
    "                                labels=labels\n",
    "                            )\n",
    "                            loss = outputs[0]\n",
    "\n",
    "                            val_losses.append(loss.detach().cpu().item())\n",
    "\n",
    "                            acc = accuracy(outputs.logits, labels.data, topk=(1,))\n",
    "                            val_accs.append(acc[0].detach().cpu().item())\n",
    "\n",
    "                    net_hist[\"val_loss\"].append(np.mean(val_losses))\n",
    "                    net_hist[\"val_x\"].append(total_steps)\n",
    "                    net_hist[\"val_acc_top_1\"].append(np.mean(val_accs))\n",
    "\n",
    "                    net.train()\n",
    "\n",
    "                if total_steps % 100 == 0:\n",
    "                    display.clear_output(wait=True)\n",
    "\n",
    "                    grouped_hist = group_uniques_full(hist, [\"train_loss\", \"val_loss\", \"val_acc_top_1\", \"train_acc_top_1\"])\n",
    "\n",
    "                    fig = plt.figure(figsize=(15, 8 + 2 * ((len(grouped_hist) + 2) // 3)))\n",
    "                    gs = GridSpec(4 + 2 * ((len(grouped_hist) + 2) // 3),3, figure=fig)\n",
    "\n",
    "                    ax1 = fig.add_subplot(gs[0:4,:2])\n",
    "                    ax2 = fig.add_subplot(gs[0:2,2])\n",
    "                    ax3 = fig.add_subplot(gs[2:4,2])\n",
    "\n",
    "                    ax1 = make_loss_plot(ax1, grouped_hist, eps=0.01, make_val=False, alpha=0.9)\n",
    "                    ax2 = make_accuracy_plot(ax2, grouped_hist, eps=0.01, make_train=True, make_val=False, top_k=1, alpha=0.9)\n",
    "                    ax3 = make_accuracy_plot(ax3, grouped_hist, eps=0.01, make_train=False, make_val=True, top_k=1, alpha=0.9)\n",
    "\n",
    "                    if calc_norm_diffs == True:\n",
    "                        draw_norm_hists_for_different_models(fig, gs[4:,:], grouped_hist, bins_n=100, draw_normal=True)\n",
    "\n",
    "                    gs.tight_layout(fig)\n",
    "                    plt.draw()\n",
    "                    plt.show()\n",
    "\n",
    "                total_steps += 1\n",
    "        net_hist[\"total_steps\"] = total_steps\n",
    "        net.to(\"cpu\")\n",
    "        optimizer_to(optimizer, \"cpu\")\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
