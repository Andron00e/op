{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a2aedb9-2c83-45d9-88f0-09fb2397718d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "098fa562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]='PCI_BUS_ID'\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = 'false'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbb420ec-caba-48ed-a6fc-2b2ec09bdcfd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreisemenov/anaconda3/envs/secondord/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import multiprocessing\n",
    "from itertools import chain\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ee5d3f6-a086-4cf0-8675-e608d56aec01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class HessianEstimator(ABC):\n",
    "    @abstractmethod\n",
    "    def estimate(self, p, grad):\n",
    "        pass\n",
    "\n",
    "\n",
    "class HutchinsonEstimator(HessianEstimator):\n",
    "    def estimate(self, p, grad):\n",
    "        u = torch.randn_like(grad)\n",
    "        grad_dot_u = torch.sum(grad * u)\n",
    "        hessian_vector_product = torch.autograd.grad(grad_dot_u, p, retain_graph=True)[0]\n",
    "        return u * hessian_vector_product\n",
    "    \n",
    "\n",
    "class GaussNewtonBartlettEstimator(HessianEstimator):\n",
    "    def __init__(self, model, input_data, loss_function):\n",
    "        self.model = model\n",
    "        self.input_data = input_data\n",
    "        self.loss_function = loss_function\n",
    "    \n",
    "    def estimate(self, p, grad):\n",
    "        B = len(self.input_data)\n",
    "        logits = [self.model(xb) for xb in self.input_data]\n",
    "        y_hats = [torch.softmax(logit, dim=0) for logit in logits]\n",
    "        g_hat = torch.autograd.grad(sum([self.loss_function(logit, y_hat) for logit, y_hat in zip(logits, y_hats)]) / B, p, retain_graph=True)[0]\n",
    "        return B * g_hat * g_hat\n",
    "    \n",
    "\n",
    "class DecoupledSophia(Optimizer):\n",
    "    def __init__(self, params, hessian_estimator, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, k=10, rho=1):\n",
    "        self.hessian_estimator = hessian_estimator\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, k=k, rho=rho)\n",
    "        super(DecoupledSophia, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            if closure is not None:\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.params_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "            grad = p.grad.data\n",
    "            if grad.is_sparse:\n",
    "                raise RuntimeError('DecoupledSophia does not support sparse gradients')\n",
    "            \n",
    "            state = self.state[p]\n",
    "\n",
    "            #state init\n",
    "            if len(state) == 0:\n",
    "                state['step'] = 0\n",
    "                state['m'] = torch.zeros_like(p.data)\n",
    "                state['h'] = torch.zeros_like(p.data)\n",
    "\n",
    "            m, h = state['m'], state['h']\n",
    "            beta1, beta2 = group['betas']\n",
    "            state['step'] += 1\n",
    "\n",
    "            if group['weight_decay'] != 0:\n",
    "                grad = grad.add(group['weight_decay'], p.data)\n",
    "\n",
    "\n",
    "            #update biased first moment estimate\n",
    "            m.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "            #update hessian estomate\n",
    "            if state['step'] % group['k'] == 1:\n",
    "                hessian_estimator = self.hessian_estimator.estimate(p, grad)\n",
    "                h.mul_(beta2).add_(1 - beta2, hessian_estimator)\n",
    "\n",
    "            #update params\n",
    "            p.data.add_(-group['lr'] * group['weight_decay'], p.data)\n",
    "            p.data.addcdiv_(-group['lr'], m, h.add(group['eps']).clamp(max=group['rho']))\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d4e4459-0f30-4f7b-ab2d-76b119e71964",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CFG:\n",
    "    SEQ_LEN: int = 1024\n",
    "    NUM_CPU: int = multiprocessing.cpu_count()\n",
    "    TOKENIZER: str = \"gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.TOKENIZER)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d0517f2-af47-4359-98a2-4fe84baa10eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-25 15:33:09.739807: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-25 15:33:09.739931: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-25 15:33:09.753336: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-25 15:33:12.613497: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-25 15:33:18.306007: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e8371cb08f4847a29f51a154e13431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "803afd071d2f4928beea975a39d534c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a739e82662f40ac9d1ce5caa7b56340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33321682dfd14349b20d38a25d5c92b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65645663d8cf4e6996e36f3aafd2265d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6d04e2f97f41968d1b248e00214d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be875e07bab48158f892193172943d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c1d8d5482e4580a2ba08f0ecea7d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/157M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d3fe6d2ab04fbc9a25a18b20b12894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/157M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409a854213b24ad08098383420b0d5b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19393865a4174d51bf68683cd3878923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0847382118d4cb088917fb27dee90c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1801350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a18ae1557b4bcfa5d116e6b0db54bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8444e7e982ca4d9da2008660e0d106f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=48):   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ccd9b3791014564a108f0109bbfb75a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=48):   0%|          | 0/1801350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1063 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1369 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1132 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1581 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "078cf6fd1ce94a23abbcadf06f04917f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=48):   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dataset = load_dataset(\"openwebtext\")\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], add_special_tokens=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=CFG.NUM_CPU,\n",
    "    remove_columns=[\"text\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31e8d1e4-30e0-49a0-9e4b-92e6a9048e95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b74a45dd8e244eff9a2bf51fc30e6acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=48):   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e718b747ce7b4bc394025a836e8291d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=48):   0%|          | 0/1801350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c784919956746a8a01ed2d15d023ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=48):   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "block_size = CFG.SEQ_LEN\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "train_dataset = tokenized_dataset.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    num_proc=CFG.NUM_CPU,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3084fbb-85aa-4f70-82e4-79d92ec3495b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the GPT-2 model and tokenizer\n",
    "config = GPT2Config.from_pretrained(\"gpt2\", n_ctx=1024)\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=config)\n",
    "\n",
    "# Choose a Hessian estimator\n",
    "hessian_estimator = HutchinsonEstimator()\n",
    "\n",
    "# Initialize the DecoupledSophia optimizer\n",
    "optimizer = DecoupledSophia(model.parameters(), hessian_estimator, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1492c46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 124,439,808 || all params: 124,439,808 || trainable%: 100.00\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params:,} || all params: {all_param:,} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf262532-e687-4344-bca8-ea90cb0b3796",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset from disk ...\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "print(\"loading dataset from disk ...\")\n",
    "train_dataset = DatasetDict.load_from_disk(\"./tests/wikitext/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6dde52f-d5ff-41fd-b8d7-62d7f322c0b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\".output\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=480,\n",
    "    save_steps=10000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    gradient_accumulation_steps=1,\n",
    "    max_grad_norm=1.0,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=2000,\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53f5727a-ff17-4720-9345-519891c78c80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "    train_dataset=train_dataset[\"train\"],\n",
    "    optimizers=(optimizer, None),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034fa919-8a4c-4377-9839-3218e4b66de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a461664f-145d-4ee4-81c8-228b09369d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {torch.exp(torch.tensor(eval_results['eval_loss']))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
